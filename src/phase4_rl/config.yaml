# Phase 4 RL Configuration
# Shared configuration for RL training, inference, and Roman's sandbox integration

# Environment configuration
environment:
  num_missiles: 1
  num_interceptors: 1
  observation_dim: 34  # Based on existing successful training
  action_dim: 6        # Based on existing successful training  
  max_episode_steps: 1000
  render_mode: "human"

# Radar system configuration
radar:
  range: 1000.0
  noise_level: 0.05
  update_rate: 10
  ground_radar_positions:
    - [0, 0]
    - [500, 500]
  onboard_radar_range: 200.0

# Spawn area configuration
spawn:
  missile_spawn_area:
    - [-100, -100]
    - [100, 100]
  interceptor_spawn_area:
    - [400, 400] 
    - [600, 600]
  target_area:
    - [800, 800]
    - [1000, 1000]

# Environmental conditions
environment_conditions:
  wind_speed: 5.0
  wind_direction: 0.0  # degrees
  wind_variability: 0.1
  atmospheric_density: 1.0

# Training configuration (based on existing successful setup)
training:
  algorithm: "PPO"
  total_timesteps: 1000000
  checkpoint_interval: 10000
  n_envs: 8
  learning_rate: 0.0003
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5

# Checkpointing and logging
checkpointing:
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  tensorboard_dir: "logs/tensorboard"
  save_replay_buffer: false
  save_vec_normalize: true

# Inference configuration
inference:
  num_episodes: 100
  render: false
  real_time: false
  export_diagnostics: true
  video_recording: false