# Terminal Specialist FINE-TUNING Configuration
# OBJECTIVE: Fine-tune existing terminal specialist for sub-50m precision
#
# KEY APPROACH:
# - Load the working terminal specialist (20251126_224936 - gets 35% success at 100m)
# - Keep the same VecNormalize stats for compatibility with HRL system
# - Use lower learning rate (fine-tuning, not training from scratch)
# - Tighten curriculum more gently (150m -> 50m)

# Inherit base settings
parent_config: ../../config.yaml

# Specialist-specific training
training:
  total_timesteps: 1000000  # 1M steps for fine-tuning
  n_envs: 12
  n_steps: 2048
  batch_size: 512
  use_subproc: false  # DummyVecEnv + GPU is faster and avoids CUDA conflicts

  # LOWER learning rate for fine-tuning (don't destroy learned weights)
  learning_rate: 0.00005  # 10x lower than fresh training
  gamma: 0.99
  gae_lambda: 0.95

  # Lower entropy for fine-tuning (less exploration needed)
  ent_coef: 0.005

  # Network architecture (must match original)
  net_arch: [512, 512, 256]
  use_lstm: false
  use_layer_norm: true
  use_orthogonal_init: true
  frame_stack: 4

  # More frequent evaluation to catch improvements
  eval_freq: 25000
  n_eval_episodes: 20
  checkpoint_freq: 50000

# PRECISION CURRICULUM: 150m -> 50m over 1M steps
# Start higher to not immediately punish existing capability
curriculum:
  enabled: true
  initial_radius: 150.0   # Start above current capability (gives room to warm up)
  final_radius: 50.0      # Target: 50m precision
  curriculum_steps: 1000000  # Match total_timesteps

  # Progression timeline (gentler than fresh training):
  # Step 0:        150m (above current capability)
  # Step 250k:     125m
  # Step 500k:     100m (current capability level)
  # Step 750k:     75m
  # Step 1000k:    50m  (target precision)

# Environment - use standard config
environment:
  max_steps: 2000
